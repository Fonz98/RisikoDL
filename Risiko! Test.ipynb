{"cells":[{"cell_type":"markdown","id":"880f2960","metadata":{"id":"880f2960"},"source":["# <b>Deep Learning:</b> Risiko! Army detector\n","\n","---\n","A.A. 2022/23 (6 CFU) - Dr. Daniel Fusaro Ph.D. Student\n","\n","---\n","\n","# Yolov7 test\n","\n","\n","With this Jupyter Notebook you can:\n"," - test the accuracy of a pre-trained Yolov7 model on your custom dataset\n"," - get the performance metrics (that you need to include in your project report)\n"," - visualize the results\n","\n","---\n","\n","Risiko! game official Wikipedia page -\n","[https://en.wikipedia.org/wiki/RisiKo!](https://en.wikipedia.org/wiki/RisiKo!)\n","\n","---\n","\n","### To run this Jupyter Notebook you will need\n","\n","#### run the following commands:\n","```\n","pip3 install torch torchvision\n","pip3 install seaborn\n","git clone https://github.com/WongKinYiu/yolov7\n"," ```\n","#### follow these steps:\n"," - the weights of the pretrained model (see the Google Drive folder of the project)\n"," - the dataset produced by you (using Risiko! Synthetic Dataset Creator.ipynb)\n"," - the dataset of real images that you find in the Google Drive folder of the project\n"," - modify the coco_risiko.yaml config file: it must contain, in the \"test\" value, the path to the test.txt file containing all the test images path\n"," - produce the aforementioned test.txt file, like the one you find in the Google Drive folder (aka test_example.txt)\n"]},{"cell_type":"code","source":["!pip install torch torchvision\n","!pip install seaborn\n","!git clone https://github.com/WongKinYiu/yolov7\n","!pip install PyQt5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ri-NZdJ_RBgU","executionInfo":{"status":"ok","timestamp":1689262053773,"user_tz":-120,"elapsed":27865,"user":{"displayName":"Andrea Fonsato","userId":"05922717929612106150"}},"outputId":"99eb79c6-1918-4731-f889-e775a58de885"},"id":"ri-NZdJ_RBgU","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n","Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.22.4)\n","Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.40.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n","Cloning into 'yolov7'...\n","remote: Enumerating objects: 1191, done.\u001b[K\n","remote: Counting objects: 100% (6/6), done.\u001b[K\n","remote: Compressing objects: 100% (4/4), done.\u001b[K\n","remote: Total 1191 (delta 2), reused 5 (delta 2), pack-reused 1185\u001b[K\n","Receiving objects: 100% (1191/1191), 74.23 MiB | 16.03 MiB/s, done.\n","Resolving deltas: 100% (514/514), done.\n","Collecting PyQt5\n","  Downloading PyQt5-5.15.9-cp37-abi3-manylinux_2_17_x86_64.whl (8.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyQt5-sip<13,>=12.11 (from PyQt5)\n","  Downloading PyQt5_sip-12.12.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.whl (360 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.5/360.5 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyQt5-Qt5>=5.15.2 (from PyQt5)\n","  Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyQt5-Qt5, PyQt5-sip, PyQt5\n","Successfully installed PyQt5-5.15.9 PyQt5-Qt5-5.15.2 PyQt5-sip-12.12.1\n"]}]},{"cell_type":"code","execution_count":2,"id":"6c15c479","metadata":{"id":"6c15c479","executionInfo":{"status":"ok","timestamp":1689262153808,"user_tz":-120,"elapsed":6435,"user":{"displayName":"Andrea Fonsato","userId":"05922717929612106150"}}},"outputs":[],"source":["import argparse\n","import json\n","import os\n","from pathlib import Path\n","from threading import Thread\n","\n","import numpy as np\n","import torch                    # if you don't have it, just \"pip3 install torch torchvision\"\n","import yaml\n","from tqdm import tqdm\n","\n","import matplotlib\n","from matplotlib import pyplot as plt\n","import cv2\n","import yaml\n","\n","import sys\n","sys.path.append('yolov7')\n","\n","# probably, you also must do \"pip3 install seaborn\"\n","\n","from models.experimental import attempt_load\n","from utils.datasets import create_dataloader\n","from utils.general import coco80_to_coco91_class, check_dataset, check_file, check_img_size, check_requirements, \\\n","    box_iou, non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, set_logging, increment_path, colorstr\n","from utils.metrics import ap_per_class, ConfusionMatrix\n","from utils.plots import plot_images, output_to_target, plot_study_txt\n","from utils.torch_utils import select_device, time_synchronized, TracedModel"]},{"cell_type":"markdown","id":"41e5fe15","metadata":{"id":"41e5fe15"},"source":["## The \"test\" function"]},{"cell_type":"code","execution_count":3,"id":"2292ff85","metadata":{"id":"2292ff85","executionInfo":{"status":"ok","timestamp":1689262155998,"user_tz":-120,"elapsed":662,"user":{"displayName":"Andrea Fonsato","userId":"05922717929612106150"}}},"outputs":[],"source":["def test(data,\n","         weights=None,\n","         batch_size=32,\n","         imgsz=640,\n","         conf_thres=0.001,\n","         iou_thres=0.6,      # for NMS\n","         save_json=False,\n","         single_cls=False,\n","         augment=False,\n","         verbose=False,\n","         model=None,\n","         dataloader=None,\n","         save_dir=Path(''),  # for saving images\n","         save_txt=False,     # for auto-labelling\n","         save_hybrid=False,  # for hybrid auto-labelling\n","         save_conf=False,    # save auto-label confidences\n","         plots=True,\n","         wandb_logger=None,\n","         compute_loss=None,\n","         half_precision=True,\n","         trace=False,\n","         is_coco=False,\n","         v5_metric=False):\n","\n","    set_logging()\n","    device = select_device(opt.device, batch_size=batch_size)\n","\n","    # Directories\n","    save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n","    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n","\n","    # Load model\n","    model = attempt_load(weights, map_location=device)  # load FP32 model\n","    gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n","    imgsz = check_img_size(imgsz, s=gs)  # check img_size\n","\n","    if trace:\n","        model = TracedModel(model, device, imgsz)\n","\n","    # Half\n","    half = device.type != 'cpu' and half_precision  # half precision only supported on CUDA\n","    if half:\n","        model.half()\n","\n","    # Configure\n","    model.eval()\n","    if isinstance(data, str):\n","        is_coco = data.endswith('coco.yaml')\n","        with open(data) as f:\n","            data = yaml.load(f, Loader=yaml.SafeLoader)\n","    check_dataset(data)  # check\n","    nc = 1 if single_cls else int(data['nc'])  # number of classes\n","    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n","    niou = iouv.numel()\n","\n","    # Logging\n","    log_imgs = 0\n","    if wandb_logger and wandb_logger.wandb:\n","        log_imgs = min(wandb_logger.log_imgs, 100)\n","    # Dataloader\n","    if device.type != 'cpu':\n","        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n","    task = opt.task if opt.task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\n","    dataloader = create_dataloader(data[task], imgsz, batch_size, gs, opt, pad=0.5, rect=True,\n","                                   prefix=colorstr(f'{task}: '))[0]\n","\n","    if v5_metric:\n","        print(\"Testing with YOLOv5 AP metric...\")\n","\n","    seen = 0\n","    confusion_matrix = ConfusionMatrix(nc=nc)\n","    names = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n","    coco91class = coco80_to_coco91_class()\n","    s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n","    p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n","    loss = torch.zeros(3, device=device)\n","    jdict, stats, ap, ap_class, wandb_images = [], [], [], [], []\n","    for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n","        img = img.to(device, non_blocking=True)\n","        img = img.half() if half else img.float()  # uint8 to fp16/32\n","        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","        targets = targets.to(device)\n","        nb, _, height, width = img.shape  # batch size, channels, height, width\n","\n","        with torch.no_grad():\n","            # Run model\n","            t = time_synchronized()\n","            out, train_out = model(img, augment=augment)  # inference and training outputs\n","            t0 += time_synchronized() - t\n","\n","            # Compute loss\n","            if compute_loss:\n","                loss += compute_loss([x.float() for x in train_out], targets)[1][:3]  # box, obj, cls\n","\n","            # Run NMS\n","            targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\n","            lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling\n","\n","            t = time_synchronized()\n","            out = non_max_suppression(out, conf_thres=conf_thres, iou_thres=iou_thres, labels=lb, multi_label=True)\n","            t1 += time_synchronized() - t\n","\n","        # Statistics per image\n","        for si, pred in enumerate(out):\n","            labels = targets[targets[:, 0] == si, 1:]\n","            nl = len(labels)\n","            tcls = labels[:, 0].tolist() if nl else []  # target class\n","            path = Path(paths[si])\n","            seen += 1\n","\n","            if len(pred) == 0:\n","                if nl:\n","                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n","                continue\n","\n","            # Predictions\n","            predn = pred.clone()\n","            scale_coords(img[si].shape[1:], predn[:, :4], shapes[si][0], shapes[si][1])  # native-space pred\n","\n","            if opt.print_bbox:\n","                for line in predn:\n","                    x, y, w, h, conf, cl = line.detach().cpu().float()\n","                    print(f\"{int(x):<6.0f} {int(y):<6.0f} {int(w):<6.0f} {int(h):<6.0f} {conf:<6.4} {int(cl):<2d}\")\n","\n","            opt.labels.append([predn, paths[si]])\n","\n","            # Append to text file\n","            if save_txt:\n","                gn = torch.tensor(shapes[si][0])[[1, 0, 1, 0]]  # normalization gain whwh\n","                for *xyxy, conf, cls in predn.tolist():\n","                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n","                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n","                    with open(save_dir / 'labels' / (path.stem + '.txt'), 'a') as f:\n","                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n","\n","            # W&B logging - Media Panel Plots\n","            if len(wandb_images) < log_imgs and wandb_logger.current_epoch > 0:  # Check for test operation\n","                if wandb_logger.current_epoch % wandb_logger.bbox_interval == 0:\n","                    box_data = [{\"position\": {\"minX\": xyxy[0], \"minY\": xyxy[1], \"maxX\": xyxy[2], \"maxY\": xyxy[3]},\n","                                 \"class_id\": int(cls),\n","                                 \"box_caption\": \"%s %.3f\" % (names[cls], conf),\n","                                 \"scores\": {\"class_score\": conf},\n","                                 \"domain\": \"pixel\"} for *xyxy, conf, cls in pred.tolist()]\n","                    boxes = {\"predictions\": {\"box_data\": box_data, \"class_labels\": names}}  # inference-space\n","                    wandb_images.append(wandb_logger.wandb.Image(img[si], boxes=boxes, caption=path.name))\n","            wandb_logger.log_training_progress(predn, path, names) if wandb_logger and wandb_logger.wandb_run else None\n","\n","            # Append to pycocotools JSON dictionary\n","            if save_json:\n","                # [{\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}, ...\n","                image_id = int(path.stem) if path.stem.isnumeric() else path.stem\n","                box = xyxy2xywh(predn[:, :4])  # xywh\n","                box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n","                for p, b in zip(pred.tolist(), box.tolist()):\n","                    jdict.append({'image_id': image_id,\n","                                  'category_id': coco91class[int(p[5])] if is_coco else int(p[5]),\n","                                  'bbox': [round(x, 3) for x in b],\n","                                  'score': round(p[4], 5)})\n","\n","            # Assign all predictions as incorrect\n","            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n","            if nl:\n","                detected = []  # target indices\n","                tcls_tensor = labels[:, 0]\n","\n","                # target boxes\n","                tbox = xywh2xyxy(labels[:, 1:5])\n","                scale_coords(img[si].shape[1:], tbox, shapes[si][0], shapes[si][1])  # native-space labels\n","                if plots:\n","                    confusion_matrix.process_batch(predn, torch.cat((labels[:, 0:1], tbox), 1))\n","\n","                # Per target class\n","                for cls in torch.unique(tcls_tensor):\n","                    ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)  # prediction indices\n","                    pi = (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1)  # target indices\n","\n","                    # Search for detections\n","                    if pi.shape[0]:\n","                        # Prediction to target ious\n","                        ious, i = box_iou(predn[pi, :4], tbox[ti]).max(1)  # best ious, indices\n","\n","                        # Append detections\n","                        detected_set = set()\n","                        for j in (ious > iouv[0]).nonzero(as_tuple=False):\n","                            d = ti[i[j]]  # detected target\n","                            if d.item() not in detected_set:\n","                                detected_set.add(d.item())\n","                                detected.append(d)\n","                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n","                                if len(detected) == nl:  # all targets already located in image\n","                                    break\n","            # Append statistics (correct, conf, pcls, tcls)\n","            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n","\n","        # Plot images\n","        if plots and batch_i < 3:\n","            f = save_dir / f'test_batch{batch_i}_labels.jpg'  # labels\n","            Thread(target=plot_images, args=(img, targets, paths, f, names), daemon=True).start()\n","            f = save_dir / f'test_batch{batch_i}_pred.jpg'  # predictions\n","            Thread(target=plot_images, args=(img, output_to_target(out), paths, f, names), daemon=True).start()\n","\n","    # Compute statistics\n","    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n","    if len(stats) and stats[0].any():\n","        p, r, ap, f1, ap_class = ap_per_class(*stats, plot=plots, v5_metric=v5_metric, save_dir=save_dir, names=names)\n","        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n","        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n","        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n","    else:\n","        nt = torch.zeros(1)\n","\n","    # Print results\n","    pf = '%20s' + '%12i' * 2 + '%12.3g' * 4  # print format\n","    print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n","\n","    # Print results per class\n","    if (verbose or nc < 50) and nc > 1 and len(stats):\n","        for i, c in enumerate(ap_class):\n","            print(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n","\n","    # Print speeds\n","    t = tuple(x / seen * 1E3 for x in (t0, t1, t0 + t1)) + (imgsz, imgsz, batch_size)  # tuple\n","    print('Speed: %.1f/%.1f/%.1f ms inference/NMS/total per %gx%g image at batch-size %g' % t)\n","\n","    # Plots\n","    if plots:\n","        confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n","        if wandb_logger and wandb_logger.wandb:\n","            val_batches = [wandb_logger.wandb.Image(str(f), caption=f.name) for f in sorted(save_dir.glob('test*.jpg'))]\n","            wandb_logger.log({\"Validation\": val_batches})\n","    if wandb_images:\n","        wandb_logger.log({\"Bounding Box Debugger/Images\": wandb_images})\n","\n","    # Save JSON\n","    if save_json and len(jdict):\n","        w = Path(weights[0] if isinstance(weights, list) else weights).stem if weights is not None else ''  # weights\n","        anno_json = './coco/annotations/instances_val2017.json'  # annotations json\n","        pred_json = str(save_dir / f\"{w}_predictions.json\")  # predictions json\n","        print('\\nEvaluating pycocotools mAP... saving %s...' % pred_json)\n","        with open(pred_json, 'w') as f:\n","            json.dump(jdict, f)\n","\n","        try:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\n","            from pycocotools.coco import COCO\n","            from pycocotools.cocoeval import COCOeval\n","\n","            anno = COCO(anno_json)  # init annotations api\n","            pred = anno.loadRes(pred_json)  # init predictions api\n","            eval = COCOeval(anno, pred, 'bbox')\n","            if is_coco:\n","                eval.params.imgIds = [int(Path(x).stem) for x in dataloader.dataset.img_files]  # image IDs to evaluate\n","            eval.evaluate()\n","            eval.accumulate()\n","            eval.summarize()\n","            map, map50 = eval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)\n","        except Exception as e:\n","            print(f'pycocotools unable to run: {e}')\n","\n","    # Return results\n","    s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n","    print(f\"Results saved to {save_dir}{s}\")\n","\n","    maps = np.zeros(nc) + map\n","    for i, c in enumerate(ap_class):\n","        maps[c] = ap[i]\n","    return (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()), maps, t"]},{"cell_type":"markdown","id":"96fc6471","metadata":{"id":"96fc6471"},"source":["# Main"]},{"cell_type":"code","source":["# mount the drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","# access the folder containing the files required to run the project\n","%cd /content/drive/My Drive/Colab environments/Risiko! DL/\n","# check that we are in the desired folder and that all required files are present\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-g4BKcL4SWME","executionInfo":{"status":"ok","timestamp":1689262177302,"user_tz":-120,"elapsed":15796,"user":{"displayName":"Andrea Fonsato","userId":"05922717929612106150"}},"outputId":"c053300b-60fe-48f2-ccfe-7bc27ae31163"},"id":"-g4BKcL4SWME","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Colab environments/Risiko! DL\n"," \u001b[0m\u001b[01;34m3D_models\u001b[0m/                                 \u001b[01;34mruns\u001b[0m/\n"," \u001b[01;34mbackgrounds\u001b[0m/                               \u001b[01;34msynthetic_dataset\u001b[0m/\n"," coco_risiko.yaml                           \u001b[01;34msynthetic_images\u001b[0m/\n"," custom_complete_yolo.yaml                  tanks_flags_detection.ipynb\n"," custom_yolo.yaml                           test.cache\n"," \u001b[01;34mdatasets\u001b[0m/                                  test_example.txt\n"," \u001b[01;34mpre_trained_weights\u001b[0m/                       test.txt\n"," \u001b[01;34mreal_images\u001b[0m/                               traced_model.pt\n","'Risiko! Synthetic Dataset Creator.ipynb'   \u001b[01;34myolov5\u001b[0m/\n","'Risiko! Test.ipynb'\n"]}]},{"cell_type":"code","execution_count":5,"id":"f86f8996","metadata":{"id":"f86f8996","executionInfo":{"status":"ok","timestamp":1689262180535,"user_tz":-120,"elapsed":618,"user":{"displayName":"Andrea Fonsato","userId":"05922717929612106150"}}},"outputs":[],"source":["class Opt:\n","    data        = \"coco_risiko.yaml\"\n","    weights     =  \"pre_trained_weights/pre_trained_yolo_weights_v00.pt\" ##### PUT HERE the path to the weights of yolo (e.h. pre_trained_weights/pre_trained_yolo_weights_v00.pt)\n","\n","    batch_size  = 1                           # size of each image batch\n","    img_size    = 640                         # inference size (pixels)\n","    conf_thres  = 0.2                         # object confidence threshold\n","    iou_thres   = 0.65                        # IOU threshold for NMS\n","    save_json   = False                       # save a cocoapi-compatible JSON results file\n","    single_cls  = False\n","    augment     = False\n","    verbose     = False\n","    save_txt    = False                        # save results to *.txt\n","    save_hybrid = False                        # save label+prediction hybrid results to *.txt\n","    save_conf   = False                        # save confidences in --save-txt labels\n","    project     = \"runs/test\"\n","    no_trace    = False                        # don`t trace model\n","    v5_metric   = False                        # assume maximum recall as 1.0 in AP calculation\n","    exist_ok    = False                        # existing project/name ok, do not increment\n","    device      = \"0\"                         # cuda device, i.e. 0 or 0,1,2,3 or cpu\n","    task        = \"test\"                      #\n","    name        = \"yolov7_risiko_test\"        # save to project/name\n","\n","    print_bbox  = False\n","\n","    labels = []"]},{"cell_type":"code","execution_count":6,"id":"f9456b1a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9456b1a","executionInfo":{"status":"ok","timestamp":1689262835575,"user_tz":-120,"elapsed":652647,"user":{"displayName":"Andrea Fonsato","userId":"05922717929612106150"}},"outputId":"9c577c46-c610-43bc-c61b-23b04e0cb88f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n"," Convert model to Traced-model... \n"," traced_script_module saved! \n"," model is traced! \n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning '/content/drive/My Drive/Colab environments/Risiko! DL/test.cache' images and labels... 1041 found, 0 missing, 0 empty, 4 corrupted: 100%|██████████| 1045/1045 [00:00<?, ?it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 1041/1041 [10:18<00:00,  1.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                 all        1041       55636        0.94       0.705       0.719       0.607\n","           blue_army        1041        7597       0.959       0.663       0.685       0.563\n","            red_army        1041        7533       0.967       0.715       0.732       0.607\n","         yellow_army        1041        7445       0.971       0.685       0.704        0.58\n","         purple_army        1041        7174       0.963       0.736       0.754       0.627\n","          black_army        1041        7484        0.94       0.677       0.694       0.559\n","          green_army        1041        7396       0.899       0.731       0.727       0.612\n","           blue_flag        1041        1928       0.957       0.628       0.655       0.563\n","            red_flag        1041        1731       0.948       0.731       0.746        0.66\n","         yellow_flag        1041        1897       0.961       0.707       0.728       0.617\n","         purple_flag        1041        1826       0.943       0.745       0.762       0.661\n","          black_flag        1041        1823       0.938       0.701       0.721       0.615\n","          green_flag        1041        1802       0.838       0.739       0.716       0.621\n","Speed: 18.7/1.2/19.9 ms inference/NMS/total per 640x640 image at batch-size 1\n","Results saved to runs/test/yolov7_risiko_test\n"]},{"output_type":"execute_result","data":{"text/plain":["((0.9404097288352418,\n","  0.7048231359722344,\n","  0.7186942683946613,\n","  0.6071042449426949,\n","  0.0,\n","  0.0,\n","  0.0),\n"," array([     0.5626,     0.60678,     0.58031,     0.62656,      0.5591,      0.6118,     0.56348,     0.66025,     0.61717,     0.66138,     0.61491,     0.62092]),\n"," (18.699037238569012, 1.22716584878971, 19.926203087358722, 640, 640, 1))"]},"metadata":{},"execution_count":6}],"source":["opt = Opt()\n","\n","opt.data = check_file(opt.data)\n","\n","test(opt.data,\n","     opt.weights,\n","     opt.batch_size,\n","     opt.img_size,\n","     opt.conf_thres,\n","     opt.iou_thres,\n","     opt.save_json,\n","     opt.single_cls,\n","     opt.augment,\n","     opt.verbose,\n","     save_txt = opt.save_txt | opt.save_hybrid,\n","     save_hybrid = opt.save_hybrid,\n","     save_conf = opt.save_conf,\n","     trace = not opt.no_trace,\n","     v5_metric = opt.v5_metric\n","     )"]},{"cell_type":"code","execution_count":null,"id":"d4c48c9b","metadata":{"id":"d4c48c9b"},"outputs":[],"source":["# We don't run this cell because we obtain some error using matplotlib.use('qtagg')\n","# We tried to install library like PyQt5 to fix this problem but the error still remain\n","# Another try was to remove the instruction matplotlib.use('qtagg') or changing it into matplotlib.use('agg') but in both cases the output was only a list\n","# of filename that is not interesting or relevant to the goal of this cell\n","matplotlib.use('qtagg')\n","\n","for labels, image_path in opt.labels:\n","    img = cv2.imread(image_path)\n","    print(image_path)\n","    for line in labels:\n","        x, y, w, h, conf, cl = line.detach().cpu().float()\n","        cv2.rectangle(img, (int(x), int(y)), (int(w), int(h)), (36,255,12, 1) if cl < 6 else (255,12,10, 1), 2)\n","    plt.imshow(img)\n","    plt.show()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[{"file_id":"1XLKlDV_MxCk3tlQf902t-UHkpO3M2vt9","timestamp":1689264348790},{"file_id":"1qyxIzo6dcieDZD-JyJR4STtoz51Il4Mf","timestamp":1689025540137}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}